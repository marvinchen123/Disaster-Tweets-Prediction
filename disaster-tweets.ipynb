{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nimport random\nimport pandas as pd\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport string\nfrom torch.utils.data import Dataset, DataLoader\nimport torchtext\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nnltk.download('stopwords')\nnltk.download(\"wordnet\")","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:45:37.527773Z","iopub.execute_input":"2023-04-27T07:45:37.528664Z","iopub.status.idle":"2023-04-27T07:45:39.443324Z","shell.execute_reply.started":"2023-04-27T07:45:37.528620Z","shell.execute_reply":"2023-04-27T07:45:39.441939Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"! unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:45:41.623435Z","iopub.execute_input":"2023-04-27T07:45:41.624036Z","iopub.status.idle":"2023-04-27T07:46:01.404263Z","shell.execute_reply.started":"2023-04-27T07:45:41.623998Z","shell.execute_reply":"2023-04-27T07:46:01.402780Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\nreplace /usr/share/nltk_data/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n","output_type":"stream"}]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    return seed\n\nseed_everything(42)\nMAX_LENGTH = 30\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:03.667963Z","iopub.execute_input":"2023-04-27T07:46:03.669242Z","iopub.status.idle":"2023-04-27T07:46:03.707151Z","shell.execute_reply.started":"2023-04-27T07:46:03.669193Z","shell.execute_reply":"2023-04-27T07:46:03.705947Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"train.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:51:42.075227Z","iopub.execute_input":"2023-04-27T07:51:42.076367Z","iopub.status.idle":"2023-04-27T07:51:42.090164Z","shell.execute_reply.started":"2023-04-27T07:51:42.076322Z","shell.execute_reply":"2023-04-27T07:51:42.088801Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0    4342\n1    3271\nName: target, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest= pd.read_csv(\"../input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:06.355608Z","iopub.execute_input":"2023-04-27T07:46:06.356043Z","iopub.status.idle":"2023-04-27T07:46:06.402586Z","shell.execute_reply.started":"2023-04-27T07:46:06.356006Z","shell.execute_reply":"2023-04-27T07:46:06.401381Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r' ', string)\n\ndef clean_sentence(sentence):\n    # remove URLS \n    sentence = re.sub(r'http\\S+', ' ', sentence)\n    # remove emoji's\n    sentence = remove_emoji(sentence)\n    # remove punctuation\n    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\" , sentence)\n    # remove double spaces\n    sentence = sentence.replace('  ',\"\")\n    \n    return sentence.strip()\n\ndef remove_stopwords(tokens, stopwords):\n    clean_token = [word for word in tokens if word not in stopwords]\n    return clean_token\n\n# lemmalization \ndef lemmatize(tokens, lemma):\n    lemmatized_tokens = [lemma.lemmatize(token, pos = 'v') for token in tokens]\n    return lemmatized_tokens","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:06.739980Z","iopub.execute_input":"2023-04-27T07:46:06.741075Z","iopub.status.idle":"2023-04-27T07:46:06.751808Z","shell.execute_reply.started":"2023-04-27T07:46:06.741026Z","shell.execute_reply":"2023-04-27T07:46:06.749444Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# fix sentence length\ndef trunc_padding(sentence):\n    modify_sentence = sentence.copy()\n    if len(modify_sentence) >= MAX_LENGTH:\n        modify_sentence = modify_sentence[:MAX_LENGTH]\n    else:\n        modify_sentence.extend(list([\"0\"] * (MAX_LENGTH - len(modify_sentence))))\n    return modify_sentence","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:06.985790Z","iopub.execute_input":"2023-04-27T07:46:06.986553Z","iopub.status.idle":"2023-04-27T07:46:06.993621Z","shell.execute_reply.started":"2023-04-27T07:46:06.986503Z","shell.execute_reply":"2023-04-27T07:46:06.992211Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')\nlemma = WordNetLemmatizer()\n\ndef processing(df, stopwords, lemma):\n    df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n    df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n    # tokenization\n    df['text'] = df['text'].apply(lambda sentence: sentence.split())\n    # remove stopwords\n    df['text'] = df['text'].apply(lambda sentence: remove_stopwords(sentence, stopwords))\n    # lemmalization \n    df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n    # sentence length before padding\n    df['length'] = df['text'].apply(lambda x: len(x))\n    # fix sentence length\n    df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n    # sentence length after padding\n    df['length_padding'] = df['text'].apply(lambda x: len(x))\n    \nprocessing(train, stopwords, lemma)\nprocessing(test, stopwords, lemma)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:07.192896Z","iopub.execute_input":"2023-04-27T07:46:07.193695Z","iopub.status.idle":"2023-04-27T07:46:10.269619Z","shell.execute_reply.started":"2023-04-27T07:46:07.193654Z","shell.execute_reply":"2023-04-27T07:46:10.268413Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:10.272330Z","iopub.execute_input":"2023-04-27T07:46:10.273037Z","iopub.status.idle":"2023-04-27T07:46:10.294293Z","shell.execute_reply.started":"2023-04-27T07:46:10.272992Z","shell.execute_reply":"2023-04-27T07:46:10.293008Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  [deeds, reason, earthquake, may, allah, forgiv...   \n1   4     NaN      NaN  [forest, fire, near, la, ronge, sask, canada, ...   \n2   5     NaN      NaN  [residents, ask, shelter, place, notify, offic...   \n3   6     NaN      NaN  [13000, people, receive, wildfires, evacuation...   \n4   7     NaN      NaN  [get, send, photo, ruby, alaska, smoke, wildfi...   \n\n   target  length  length_padding  \n0       1       7              30  \n1       1       7              30  \n2       1      11              30  \n3       1       7              30  \n4       1       9              30  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>length</th>\n      <th>length_padding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n      <td>1</td>\n      <td>7</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[forest, fire, near, la, ronge, sask, canada, ...</td>\n      <td>1</td>\n      <td>7</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[residents, ask, shelter, place, notify, offic...</td>\n      <td>1</td>\n      <td>11</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[13000, people, receive, wildfires, evacuation...</td>\n      <td>1</td>\n      <td>7</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[get, send, photo, ruby, alaska, smoke, wildfi...</td>\n      <td>1</td>\n      <td>9</td>\n      <td>30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:10.297197Z","iopub.execute_input":"2023-04-27T07:46:10.298063Z","iopub.status.idle":"2023-04-27T07:46:10.315916Z","shell.execute_reply.started":"2023-04-27T07:46:10.298015Z","shell.execute_reply":"2023-04-27T07:46:10.314557Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   0     NaN      NaN  [happen, terrible, car, crash, 0, 0, 0, 0, 0, ...   \n1   2     NaN      NaN  [hear, earthquake, different, cities, stay, sa...   \n2   3     NaN      NaN  [forest, fire, spot, pond, geese, flee, across...   \n3   9     NaN      NaN  [apocalypse, light, spokane, wildfires, 0, 0, ...   \n4  11     NaN      NaN  [typhoon, soudelor, kill, 28, china, taiwan, 0...   \n\n   length  length_padding  \n0       4              30  \n1       7              30  \n2      10              30  \n3       4              30  \n4       6              30  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>length</th>\n      <th>length_padding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[happen, terrible, car, crash, 0, 0, 0, 0, 0, ...</td>\n      <td>4</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[hear, earthquake, different, cities, stay, sa...</td>\n      <td>7</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[forest, fire, spot, pond, geese, flee, across...</td>\n      <td>10</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[apocalypse, light, spokane, wildfires, 0, 0, ...</td>\n      <td>4</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[typhoon, soudelor, kill, 28, china, taiwan, 0...</td>\n      <td>6</td>\n      <td>30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Assign unique index to each word, used for word embedding","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get all vocabulary\nvocab_list = []\nfor sentence in train['text']:\n    vocab_list.append(sentence)\nvocab = build_vocab_from_iterator(vocab_list, specials=[\"<unk>\"])\nvocab.set_default_index(vocab[\"<unk>\"])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:12.667572Z","iopub.execute_input":"2023-04-27T07:46:12.668161Z","iopub.status.idle":"2023-04-27T07:46:12.900087Z","shell.execute_reply.started":"2023-04-27T07:46:12.668107Z","shell.execute_reply":"2023-04-27T07:46:12.898739Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"class TweetDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y \n    \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        sentence = self.x[idx]\n        x = torch.tensor(vocab(sentence), dtype=torch.long).to(device)\n        y = torch.tensor(self.y[idx], dtype=torch.long).to(device)\n        return x, y","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:14.102930Z","iopub.execute_input":"2023-04-27T07:46:14.103933Z","iopub.status.idle":"2023-04-27T07:46:14.112541Z","shell.execute_reply.started":"2023-04-27T07:46:14.103855Z","shell.execute_reply":"2023-04-27T07:46:14.111081Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model_config = {\n    'vocab_size': len(vocab),\n    'hidden_dim' : 256,\n    'embedding_dim' : 200,\n    'num_classes' : 2,\n    'n_layers': 2,\n    'dropout': 0.2\n}","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:16.712731Z","iopub.execute_input":"2023-04-27T07:46:16.713521Z","iopub.status.idle":"2023-04-27T07:46:16.719486Z","shell.execute_reply.started":"2023-04-27T07:46:16.713476Z","shell.execute_reply":"2023-04-27T07:46:16.718066Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_data = TweetDataset(train['text'], train['target'])\ntrain_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:17.303752Z","iopub.execute_input":"2023-04-27T07:46:17.304575Z","iopub.status.idle":"2023-04-27T07:46:17.311319Z","shell.execute_reply.started":"2023-04-27T07:46:17.304519Z","shell.execute_reply":"2023-04-27T07:46:17.309819Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for batch in train_dataloader:\n    print(batch)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:19.164219Z","iopub.execute_input":"2023-04-27T07:46:19.165481Z","iopub.status.idle":"2023-04-27T07:46:20.753579Z","shell.execute_reply.started":"2023-04-27T07:46:19.165431Z","shell.execute_reply":"2023-04-27T07:46:20.752308Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[tensor([[  191,  1122,  2990,  ...,     1,     1,     1],\n        [    1,   274, 11380,  ...,     1,     1,     1],\n        [   54,   518,  4206,  ...,     1,     1,     1],\n        ...,\n        [ 4056,   605,  1295,  ...,     1,     1,     1],\n        [    3,    67,    24,  ...,     1,     1,     1],\n        [  292,   483,  3151,  ...,     1,     1,     1]], device='cuda:0'), tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0], device='cuda:0')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"markdown","source":"# LSTM + Atention","metadata":{}},{"cell_type":"code","source":"class LSTM_Attention(nn.Module):\n    def __init__(self, config):\n        super(LSTM_Attention, self).__init__()\n        self.config = config\n        self.embedding = nn.Embedding(config['vocab_size'], config['embedding_dim'])\n        self.lstm = nn.LSTM(config['embedding_dim'], config['hidden_dim'], bidirectional=True)\n        self.fc = nn.Linear(config['hidden_dim'] * 2, config['num_classes'])\n        self.sigmoid = nn.Sigmoid()\n    \n    def attention(self, lstm_output, final_state):\n        hidden = final_state.view(-1, self.config['hidden_dim'] * 2, 1)\n        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2)\n        soft_attn_weights = F.softmax(attn_weights, 1)\n        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n        return context\n    \n    def forward(self, X):\n        embeds = self.embedding(X).permute(1, 0, 2)\n        hidden_state = Variable(torch.zeros(2, len(X), self.config['hidden_dim'])).to(device)\n        cell_state = Variable(torch.zeros(2, len(X), self.config['hidden_dim'])).to(device)\n        output, (final_hidden_state, final_cell_state) = self.lstm(embeds, (hidden_state, cell_state))\n        output = output.permute(1, 0, 2)\n        attn_output = self.attention(output, final_hidden_state)\n        output = self.fc(attn_output)\n        return self.sigmoid(output)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:24.972558Z","iopub.execute_input":"2023-04-27T07:46:24.973111Z","iopub.status.idle":"2023-04-27T07:46:24.985549Z","shell.execute_reply.started":"2023-04-27T07:46:24.973070Z","shell.execute_reply":"2023-04-27T07:46:24.984178Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def binary_accuracy(preds, y):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    \"\"\"\n    prediction = preds.argmax(dim=1)\n    correct = (prediction == y).float() #convert into float for division \n    acc = correct.sum() / len(correct)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:25.888446Z","iopub.execute_input":"2023-04-27T07:46:25.888839Z","iopub.status.idle":"2023-04-27T07:46:25.895521Z","shell.execute_reply.started":"2023-04-27T07:46:25.888805Z","shell.execute_reply":"2023-04-27T07:46:25.894222Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"model = LSTM_Attention(model_config)\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.Adam(model.parameters(), lr = 0.001)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:27.266559Z","iopub.execute_input":"2023-04-27T07:46:27.267710Z","iopub.status.idle":"2023-04-27T07:46:27.685197Z","shell.execute_reply.started":"2023-04-27T07:46:27.267666Z","shell.execute_reply":"2023-04-27T07:46:27.683948Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"epoch_loss = 0\nepoch_acc = 0\nfor epoch in range(20):\n    model.train()\n    optimizer.zero_grad()\n    for batch in tqdm(train_dataloader):\n        optimizer.zero_grad()\n        inputs, target = batch\n        outputs = model(inputs)\n        loss = criterion(outputs, target)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss\n        \n        acc = binary_accuracy(outputs, target)\n        epoch_acc += acc.item()\n    \n    print(\"The training loss at epoch {} is {}; The training accuracy is {}\".format(epoch, epoch_loss / len(train_dataloader), \n                                                                                    round(epoch_acc/len(train_dataloader), 3)))\n    epoch_loss = 0\n    epoch_acc = 0","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:46:27.687763Z","iopub.execute_input":"2023-04-27T07:46:27.688287Z","iopub.status.idle":"2023-04-27T07:46:59.659583Z","shell.execute_reply.started":"2023-04-27T07:46:27.688241Z","shell.execute_reply":"2023-04-27T07:46:59.657954Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"100%|██████████| 119/119 [00:02<00:00, 56.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 0 is 0.6793618202209473; The training accuracy is 0.583\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 78.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 1 is 0.5985772609710693; The training accuracy is 0.703\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 78.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 2 is 0.532684862613678; The training accuracy is 0.774\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 78.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 3 is 0.4748460352420807; The training accuracy is 0.833\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 78.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 4 is 0.4469597637653351; The training accuracy is 0.865\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 78.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 5 is 0.4257085919380188; The training accuracy is 0.888\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 72.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 6 is 0.41365182399749756; The training accuracy is 0.9\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 78.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 7 is 0.4097965657711029; The training accuracy is 0.903\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 78.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 8 is 0.3984375596046448; The training accuracy is 0.916\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 76.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 9 is 0.39292415976524353; The training accuracy is 0.92\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 74.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 10 is 0.3886856436729431; The training accuracy is 0.924\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 77.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 11 is 0.3904612064361572; The training accuracy is 0.922\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 78.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 12 is 0.4053955674171448; The training accuracy is 0.906\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 74.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 13 is 0.38732877373695374; The training accuracy is 0.925\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 78.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 14 is 0.3787517249584198; The training accuracy is 0.934\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 64.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 15 is 0.3759597837924957; The training accuracy is 0.937\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 74.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 16 is 0.37242498993873596; The training accuracy is 0.941\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 77.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 17 is 0.3702971637248993; The training accuracy is 0.943\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 77.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 18 is 0.3687030076980591; The training accuracy is 0.944\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 119/119 [00:01<00:00, 74.82it/s]","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 19 is 0.36853986978530884; The training accuracy is 0.945\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_prediction(inputs, model):\n    inputs = torch.tensor(vocab(inputs), dtype=torch.long).to(device).unsqueeze(0)\n    outpus =  model(inputs).argmax(dim=1).item()\n    return outpus\n\ntest['target'] = test['text'].apply(lambda sentence: generate_prediction(sentence, model))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:54:30.440631Z","iopub.execute_input":"2023-04-27T07:54:30.441217Z","iopub.status.idle":"2023-04-27T07:54:34.587255Z","shell.execute_reply.started":"2023-04-27T07:54:30.441167Z","shell.execute_reply":"2023-04-27T07:54:34.586043Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"submission = test[['id', 'target']]\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:55:21.550984Z","iopub.execute_input":"2023-04-27T07:55:21.552155Z","iopub.status.idle":"2023-04-27T07:55:21.567502Z","shell.execute_reply.started":"2023-04-27T07:55:21.552110Z","shell.execute_reply":"2023-04-27T07:55:21.565952Z"},"trusted":true},"execution_count":48,"outputs":[]}]}
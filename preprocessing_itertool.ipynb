{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yijun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/yijun/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(42)\n",
    "MAX_LENGTH = 30\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('train.csv')\n",
    "test= pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proprocessing without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', string)\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # remove URLS \n",
    "    sentence = re.sub(r'http\\S+', ' ', sentence)\n",
    "    # remove emoji's\n",
    "    sentence = remove_emoji(sentence)\n",
    "    # remove punctuation\n",
    "    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\" , sentence)\n",
    "    # remove double spaces\n",
    "    sentence = sentence.replace('  ',\"\")\n",
    "    \n",
    "    return sentence.strip()\n",
    "\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    clean_token = [word for word in tokens if word not in stopwords]\n",
    "    return clean_token\n",
    "\n",
    "# lemmalization \n",
    "def lemmatize(tokens, lemma):\n",
    "    lemmatized_tokens = [lemma.lemmatize(token, pos = 'v') for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# fix sentence length\n",
    "def trunc_padding(sentence):\n",
    "    modify_sentence = sentence.copy()\n",
    "    if len(modify_sentence) >= MAX_LENGTH:\n",
    "        modify_sentence = modify_sentence[:MAX_LENGTH]\n",
    "    else:\n",
    "        modify_sentence.extend(list([\"0\"] * (MAX_LENGTH - len(modify_sentence))))\n",
    "    return modify_sentence\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def processing(df, stopwords, lemma):\n",
    "    df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n",
    "    df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
    "    # tokenization\n",
    "    df['text'] = df['text'].apply(lambda sentence: sentence.split())\n",
    "    # remove stopwords\n",
    "    df['text'] = df['text'].apply(lambda sentence: remove_stopwords(sentence, stopwords))\n",
    "    # lemmalization \n",
    "    df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n",
    "    # sentence length before padding\n",
    "    df['length'] = df['text'].apply(lambda x: len(x))\n",
    "    # fix sentence length\n",
    "    df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
    "    # sentence length after padding\n",
    "    df['length_padding'] = df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 2.91044 s\n",
      "File: /var/folders/tp/8cvkqhbd14x93r7vzb73wjw40000gn/T/ipykernel_14986/3006866641.py\n",
      "Function: processing at line 45\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    45                                           def processing(df, stopwords, lemma):\n",
      "    46         1      12039.0  12039.0      0.4      df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n",
      "    47         1     274281.0 274281.0      9.4      df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
      "    48                                               # tokenization\n",
      "    49         1      26956.0  26956.0      0.9      df['text'] = df['text'].apply(lambda sentence: sentence.split())\n",
      "    50                                               # remove stopwords\n",
      "    51         1     496703.0 496703.0     17.1      df['text'] = df['text'].apply(lambda sentence: remove_stopwords(sentence, stopwords))\n",
      "    52                                               # lemmalization \n",
      "    53         1    2043470.0 2043470.0     70.2      df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n",
      "    54                                               # sentence length before padding\n",
      "    55         1       8823.0   8823.0      0.3      df['length'] = df['text'].apply(lambda x: len(x))\n",
      "    56                                               # fix sentence length\n",
      "    57         1      39556.0  39556.0      1.4      df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
      "    58                                               # sentence length after padding\n",
      "    59         1       8614.0   8614.0      0.3      df['length_padding'] = df['text'].apply(lambda x: len(x))"
     ]
    }
   ],
   "source": [
    "%lprun -f processing processing(train, stopwords, lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 1.03425 s\n",
      "File: /var/folders/tp/8cvkqhbd14x93r7vzb73wjw40000gn/T/ipykernel_14986/3006866641.py\n",
      "Function: processing at line 45\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    45                                           def processing(df, stopwords, lemma):\n",
      "    46         1       4323.0   4323.0      0.4      df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n",
      "    47         1     121556.0 121556.0     11.8      df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
      "    48                                               # tokenization\n",
      "    49         1      12452.0  12452.0      1.2      df['text'] = df['text'].apply(lambda sentence: sentence.split())\n",
      "    50                                               # remove stopwords\n",
      "    51         1     197353.0 197353.0     19.1      df['text'] = df['text'].apply(lambda sentence: remove_stopwords(sentence, stopwords))\n",
      "    52                                               # lemmalization \n",
      "    53         1     650725.0 650725.0     62.9      df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n",
      "    54                                               # sentence length before padding\n",
      "    55         1       9194.0   9194.0      0.9      df['length'] = df['text'].apply(lambda x: len(x))\n",
      "    56                                               # fix sentence length\n",
      "    57         1      30002.0  30002.0      2.9      df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
      "    58                                               # sentence length after padding\n",
      "    59         1       8648.0   8648.0      0.8      df['length_padding'] = df['text'].apply(lambda x: len(x))"
     ]
    }
   ],
   "source": [
    "%lprun -f processing processing(test, stopwords, lemma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing with itertools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimzation: \n",
    "1. Combine function clean_sentence,remove_stopwords,lemmatize to reduce function call overhead \n",
    "2. Use itertools to reduce memory usage:\n",
    "    1) Using itertools.filterfalse to efficiently remove stopwords from the tokenized sentence\n",
    "    2) Using itertools.chain.from_iterable and itertools.islice to efficiently fix sentence length by padding or truncating the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('train.csv')\n",
    "test= pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yijun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/yijun/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import itertools\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', string)\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# combine func clean_sentence,remove_stopwords,lemmatize \n",
    "def clean_sentence(sentence): \n",
    "    sentence = re.sub(r'http\\S+', ' ', sentence) # remove urls\n",
    "    sentence = remove_emoji(sentence) # remove emojis\n",
    "    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\", sentence) # remove punctuations\n",
    "    sentence = sentence.replace('  ',\"\") # remove double spaces\n",
    "    sentence = sentence.lower().split() # lowercase and tokenize\n",
    "    sentence = itertools.filterfalse(lambda x: x in stopwords, sentence) # remove stopwords\n",
    "    sentence = list(itertools.chain.from_iterable(map(lambda x: [lemma.lemmatize(x, pos='v')], sentence))) # lemmatize\n",
    "    return sentence\n",
    "\n",
    "#fix sentence length\n",
    "def trunc_padding(sentence):\n",
    "    sentence = itertools.islice(itertools.chain(sentence, itertools.repeat('0')), 0, MAX_LENGTH) # fix sentence length\n",
    "    return list(sentence)\n",
    "\n",
    "\n",
    "def processing(df):\n",
    "    df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
    "    df['length'] = df['text'].apply(lambda x: len(x))\n",
    "    df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
    "    df['length_padding'] = MAX_LENGTH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 2.31342 s\n",
      "File: /var/folders/tp/8cvkqhbd14x93r7vzb73wjw40000gn/T/ipykernel_14986/1653265414.py\n",
      "Function: processing at line 39\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    39                                           def processing(df):\n",
      "    40         1    2275029.0 2275029.0     98.3      df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
      "    41         1       6544.0   6544.0      0.3      df['length'] = df['text'].apply(lambda x: len(x))\n",
      "    42         1      31165.0  31165.0      1.3      df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
      "    43         1        679.0    679.0      0.0      df['length_padding'] = MAX_LENGTH"
     ]
    }
   ],
   "source": [
    "%lprun -f processing processing(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 1.00814 s\n",
      "File: /var/folders/tp/8cvkqhbd14x93r7vzb73wjw40000gn/T/ipykernel_14986/1653265414.py\n",
      "Function: processing at line 39\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    39                                           def processing(df):\n",
      "    40         1     975215.0 975215.0     96.7      df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
      "    41         1       3575.0   3575.0      0.4      df['length'] = df['text'].apply(lambda x: len(x))\n",
      "    42         1      28569.0  28569.0      2.8      df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
      "    43         1        776.0    776.0      0.1      df['length_padding'] = MAX_LENGTH"
     ]
    }
   ],
   "source": [
    "%lprun -f processing processing(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison\n",
    "processing training data without optimization takes:  2.91044 s <br>\n",
    "processing training data with itertool optimization takes:  2.31342 s <br>\n",
    "The speed up is : 2.91044 / 2.31342 ≈ 1.26 <br><br>\n",
    "processing testing data without optimization takes:  1.03425 s <br>\n",
    "processing testing data with itertool optimization takes:  1.00814 s  <br>\n",
    "The speed up is : 1.03425 / 1.00814 ≈ 1.025<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

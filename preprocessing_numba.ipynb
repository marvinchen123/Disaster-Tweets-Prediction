{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/melodywang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/melodywang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(42)\n",
    "MAX_LENGTH = 30\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train= pd.read_csv('./nlp-getting-started/train.csv')\n",
    "test= pd.read_csv(\"./nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', string)\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # remove URLS \n",
    "    sentence = re.sub(r'http\\S+', ' ', sentence)\n",
    "    # remove emoji's\n",
    "    sentence = remove_emoji(sentence)\n",
    "    # remove punctuation\n",
    "    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\" , sentence)\n",
    "    # remove double spaces\n",
    "    sentence = sentence.replace('  ',\"\")\n",
    "    \n",
    "    return sentence.strip()\n",
    "\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    clean_token = [word for word in tokens if word not in stopwords]\n",
    "    return clean_token\n",
    "\n",
    "# lemmalization \n",
    "def lemmatize(tokens, lemma):\n",
    "    lemmatized_tokens = [lemma.lemmatize(token, pos = 'v') for token in tokens]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix sentence length\n",
    "def trunc_padding(sentence):\n",
    "    modify_sentence = sentence.copy()\n",
    "    if len(modify_sentence) >= MAX_LENGTH:\n",
    "        modify_sentence = modify_sentence[:MAX_LENGTH]\n",
    "    else:\n",
    "        modify_sentence.extend(list([\"0\"] * (MAX_LENGTH - len(modify_sentence))))\n",
    "    return modify_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def processing(df, stopwords, lemma):\n",
    "    df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n",
    "    df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
    "    # tokenization\n",
    "    df['text'] = df['text'].apply(lambda sentence: sentence.split())\n",
    "    # remove stopwords\n",
    "    df['text'] = df['text'].apply(lambda sentence: remove_stopwords(sentence, stopwords))\n",
    "    # lemmalization \n",
    "    df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n",
    "    # sentence length before padding\n",
    "    df['length'] = df['text'].apply(lambda x: len(x))\n",
    "    # fix sentence length\n",
    "    df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
    "    # sentence length after padding\n",
    "    df['length_padding'] = df['text'].apply(lambda x: len(x))\n",
    "    \n",
    "#processing(train, stopwords, lemma)\n",
    "#processing(test, stopwords, lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 3.84082 s\n",
       "File: /var/folders/ch/4ymmt1qs5bz_506vg4nzkmb80000gn/T/ipykernel_58776/2136084996.py\n",
       "Function: processing at line 4\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     4                                           def processing(df, stopwords, lemma):\n",
       "     5         1      20483.0  20483.0      0.5      df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n",
       "     6         1     131815.0 131815.0      3.4      df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
       "     7                                               # tokenization\n",
       "     8         1       7764.0   7764.0      0.2      df['text'] = df['text'].apply(lambda sentence: sentence.split())\n",
       "     9                                               # remove stopwords\n",
       "    10         1     192563.0 192563.0      5.0      df['text'] = df['text'].apply(lambda sentence: remove_stopwords(sentence, stopwords))\n",
       "    11                                               # lemmalization \n",
       "    12         1    3470183.0 3470183.0     90.4      df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n",
       "    13                                               # sentence length before padding\n",
       "    14         1       4064.0   4064.0      0.1      df['length'] = df['text'].apply(lambda x: len(x))\n",
       "    15                                               # fix sentence length\n",
       "    16         1      11107.0  11107.0      0.3      df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
       "    17                                               # sentence length after padding\n",
       "    18         1       2839.0   2839.0      0.1      df['length_padding'] = df['text'].apply(lambda x: len(x))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f processing processing(train, stopwords, lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba Optimization Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export NUMBA_ENABLE_CUDASIM=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NUMBA_ENABLE_CUDASIM=1\n"
     ]
    }
   ],
   "source": [
    "%env NUMBA_ENABLE_CUDASIM=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda, jit\n",
    "print(cuda.gpus)\n",
    "\n",
    "cuda.select_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', string)\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # remove URLS \n",
    "    sentence = re.sub(r'http\\S+', ' ', sentence)\n",
    "    # remove emoji's\n",
    "    sentence = remove_emoji(sentence)\n",
    "    # remove punctuation\n",
    "    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\" , sentence)\n",
    "    # remove double spaces\n",
    "    sentence = sentence.replace('  ',\"\")\n",
    "    \n",
    "    return sentence.strip()\n",
    "\n",
    "@jit(nopython=True)\n",
    "def remove_stopwords(tokens, stopwords):\n",
    "    clean_token = [t for t in tokens if t not in stopwords]\n",
    "    return list(clean_token)\n",
    "\n",
    "# lemmalization \n",
    "def lemmatize(tokens, lemma):\n",
    "    lemmatized_tokens = [lemma.lemmatize(token, pos = 'v') for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "@jit(nopython=True)\n",
    "def len_(sentence):\n",
    "    return len(sentence)\n",
    "\n",
    "@jit(nopython=True)\n",
    "def split_(sentence):\n",
    "    return sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def processing(df, stopwords, lemma):\n",
    "    df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n",
    "    df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
    "    # tokenization\n",
    "    df['text'] = df['text'].apply(lambda sentence: split_(sentence))\n",
    "    # remove stopwords\n",
    "    A = set(train['text'].sum())\n",
    "    B = set(stopwords)\n",
    "    word2id = {c:i for i,c in enumerate(A.union(B))}\n",
    "    id2word = {i:c for c,i in word2id.items()}\n",
    "    sw = [word2id.get(c) for c in stopwords]\n",
    "    train['text_coded'] = train['text'].apply(lambda x: [word2id.get(c) for c in x])\n",
    "    train['text_coded'] = train['text_coded'].apply(lambda x: remove_stopwords(x, sw)) \n",
    "    train['text'] = train['text_coded'].apply(lambda x: [id2word.get(i) for i in x])\n",
    "    # lemmalization \n",
    "    df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n",
    "    # sentence length before padding\n",
    "    df['length'] = df['text'].apply(lambda x: len_(x))\n",
    "    # fix sentence length\n",
    "    df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
    "    # sentence length after padding\n",
    "    df['length_padding'] = df['text'].apply(lambda x: len(x))\n",
    "    \n",
    "#processing(train, stopwords, lemma)\n",
    "#processing(test, stopwords, lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/numba/core/ir_utils.py:2119: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'stopwords' of function 'remove_stopwords'.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"../../../../../var/folders/ch/4ymmt1qs5bz_506vg4nzkmb80000gn/T/ipykernel_58776/2732514625.py\", line 24:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/numba/core/ir_utils.py:2119: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'tokens' of function 'remove_stopwords'.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"../../../../../var/folders/ch/4ymmt1qs5bz_506vg4nzkmb80000gn/T/ipykernel_58776/2732514625.py\", line 24:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/numba/core/ir_utils.py:2119: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'lst' of function 'in_seq.<locals>.seq_contains_impl'.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"../../../../../opt/anaconda3/lib/python3.9/site-packages/numba/cpython/listobj.py\", line 660:\u001b[0m\n",
      "\u001b[1mdef in_seq(context, builder, sig, args):\n",
      "\u001b[1m    def seq_contains_impl(lst, value):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/numba/core/ir_utils.py:2119: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'sentence' of function 'len_'.\n",
      "\n",
      "For more information visit https://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"../../../../../var/folders/ch/4ymmt1qs5bz_506vg4nzkmb80000gn/T/ipykernel_58776/2732514625.py\", line 34:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 8.71467 s\n",
       "File: /var/folders/ch/4ymmt1qs5bz_506vg4nzkmb80000gn/T/ipykernel_58776/2878100174.py\n",
       "Function: processing at line 5\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     5                                           def processing(df, stopwords, lemma):\n",
       "     6         1      14409.0  14409.0      0.2      df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n",
       "     7         1      98025.0  98025.0      1.1      df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n",
       "     8                                               # tokenization\n",
       "     9         1      57817.0  57817.0      0.7      df['text'] = df['text'].apply(lambda sentence: split_(sentence))\n",
       "    10                                               # remove stopwords\n",
       "    11         1     895906.0 895906.0     10.3      A = set(train['text'].sum())\n",
       "    12         1         11.0     11.0      0.0      B = set(stopwords)\n",
       "    13         1       3212.0   3212.0      0.0      word2id = {c:i for i,c in enumerate(A.union(B))}\n",
       "    14         1       2381.0   2381.0      0.0      id2word = {i:c for c,i in word2id.items()}\n",
       "    15         1         43.0     43.0      0.0      sw = [word2id.get(c) for c in stopwords]\n",
       "    16         1      26101.0  26101.0      0.3      train['text_coded'] = train['text'].apply(lambda x: [word2id.get(c) for c in x])\n",
       "    17         1    6447440.0 6447440.0     74.0      train['text_coded'] = train['text_coded'].apply(lambda x: remove_stopwords(x, sw)) \n",
       "    18         1      19504.0  19504.0      0.2      train['text'] = train['text_coded'].apply(lambda x: [id2word.get(i) for i in x])\n",
       "    19                                               # lemmalization \n",
       "    20         1     703200.0 703200.0      8.1      df['text'] = df['text'].apply(lambda sentence: lemmatize(sentence, lemma))\n",
       "    21                                               # sentence length before padding\n",
       "    22         1     431603.0 431603.0      5.0      df['length'] = df['text'].apply(lambda x: len_(x))\n",
       "    23                                               # fix sentence length\n",
       "    24         1      12067.0  12067.0      0.1      df['text'] = df['text'].apply(lambda sentence: trunc_padding(sentence))\n",
       "    25                                               # sentence length after padding\n",
       "    26         1       2951.0   2951.0      0.0      df['length_padding'] = df['text'].apply(lambda x: len(x))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f processing processing(train, stopwords, lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign unique index to each word, used for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T07:46:12.668161Z",
     "iopub.status.busy": "2023-04-27T07:46:12.667572Z",
     "iopub.status.idle": "2023-04-27T07:46:12.900087Z",
     "shell.execute_reply": "2023-04-27T07:46:12.898739Z",
     "shell.execute_reply.started": "2023-04-27T07:46:12.668107Z"
    }
   },
   "outputs": [],
   "source": [
    "# get all vocabulary\n",
    "vocab_list = []\n",
    "for sentence in train['text']:\n",
    "    vocab_list.append(sentence)\n",
    "vocab = build_vocab_from_iterator(vocab_list, specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T07:46:14.103933Z",
     "iopub.status.busy": "2023-04-27T07:46:14.102930Z",
     "iopub.status.idle": "2023-04-27T07:46:14.112541Z",
     "shell.execute_reply": "2023-04-27T07:46:14.111081Z",
     "shell.execute_reply.started": "2023-04-27T07:46:14.103855Z"
    }
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.x[idx]\n",
    "        x = torch.tensor(vocab(sentence), dtype=torch.long).to(device)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long).to(device)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T07:46:16.713521Z",
     "iopub.status.busy": "2023-04-27T07:46:16.712731Z",
     "iopub.status.idle": "2023-04-27T07:46:16.719486Z",
     "shell.execute_reply": "2023-04-27T07:46:16.718066Z",
     "shell.execute_reply.started": "2023-04-27T07:46:16.713476Z"
    }
   },
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'vocab_size': len(vocab),\n",
    "    'hidden_dim' : 256,\n",
    "    'embedding_dim' : 200,\n",
    "    'num_classes' : 2,\n",
    "    'n_layers': 2,\n",
    "    'dropout': 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T07:46:17.304575Z",
     "iopub.status.busy": "2023-04-27T07:46:17.303752Z",
     "iopub.status.idle": "2023-04-27T07:46:17.311319Z",
     "shell.execute_reply": "2023-04-27T07:46:17.309819Z",
     "shell.execute_reply.started": "2023-04-27T07:46:17.304519Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = TweetDataset(train['text'], train['target'])\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T07:46:19.165481Z",
     "iopub.status.busy": "2023-04-27T07:46:19.164219Z",
     "iopub.status.idle": "2023-04-27T07:46:20.753579Z",
     "shell.execute_reply": "2023-04-27T07:46:20.752308Z",
     "shell.execute_reply.started": "2023-04-27T07:46:19.165431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  191,  1122,  2990,  ...,     1,     1,     1],\n",
      "        [    1,   274, 11380,  ...,     1,     1,     1],\n",
      "        [   54,   518,  4206,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [ 4056,   605,  1295,  ...,     1,     1,     1],\n",
      "        [    3,    67,    24,  ...,     1,     1,     1],\n",
      "        [  292,   483,  3151,  ...,     1,     1,     1]], device='cuda:0'), tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + Atention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T07:46:24.973111Z",
     "iopub.status.busy": "2023-04-27T07:46:24.972558Z",
     "iopub.status.idle": "2023-04-27T07:46:24.985549Z",
     "shell.execute_reply": "2023-04-27T07:46:24.984178Z",
     "shell.execute_reply.started": "2023-04-27T07:46:24.973070Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(LSTM_Attention, self).__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['embedding_dim'])\n",
    "        self.lstm = nn.LSTM(config['embedding_dim'], config['hidden_dim'], bidirectional=True)\n",
    "        self.fc = nn.Linear(config['hidden_dim'] * 2, config['num_classes'])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def attention(self, lstm_output, final_state):\n",
    "        hidden = final_state.view(-1, self.config['hidden_dim'] * 2, 1)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return context\n",
    "    \n",
    "    def forward(self, X):\n",
    "        embeds = self.embedding(X).permute(1, 0, 2)\n",
    "        hidden_state = Variable(torch.zeros(2, len(X), self.config['hidden_dim'])).to(device)\n",
    "        cell_state = Variable(torch.zeros(2, len(X), self.config['hidden_dim'])).to(device)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(embeds, (hidden_state, cell_state))\n",
    "        output = output.permute(1, 0, 2)\n",
    "        attn_output = self.attention(output, final_hidden_state)\n",
    "        output = self.fc(attn_output)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T07:46:25.888839Z",
     "iopub.status.busy": "2023-04-27T07:46:25.888446Z",
     "iopub.status.idle": "2023-04-27T07:46:25.895521Z",
     "shell.execute_reply": "2023-04-27T07:46:25.894222Z",
     "shell.execute_reply.started": "2023-04-27T07:46:25.888805Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    prediction = preds.argmax(dim=1)\n",
    "    correct = (prediction == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T07:46:27.267710Z",
     "iopub.status.busy": "2023-04-27T07:46:27.266559Z",
     "iopub.status.idle": "2023-04-27T07:46:27.685197Z",
     "shell.execute_reply": "2023-04-27T07:46:27.683948Z",
     "shell.execute_reply.started": "2023-04-27T07:46:27.267666Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LSTM_Attention(model_config)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T07:46:27.688287Z",
     "iopub.status.busy": "2023-04-27T07:46:27.687763Z",
     "iopub.status.idle": "2023-04-27T07:46:59.659583Z",
     "shell.execute_reply": "2023-04-27T07:46:59.657954Z",
     "shell.execute_reply.started": "2023-04-27T07:46:27.688241Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:02<00:00, 56.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 0 is 0.6793618202209473; The training accuracy is 0.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 78.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 1 is 0.5985772609710693; The training accuracy is 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 78.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 2 is 0.532684862613678; The training accuracy is 0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 78.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 3 is 0.4748460352420807; The training accuracy is 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 78.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 4 is 0.4469597637653351; The training accuracy is 0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 78.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 5 is 0.4257085919380188; The training accuracy is 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 72.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 6 is 0.41365182399749756; The training accuracy is 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 78.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 7 is 0.4097965657711029; The training accuracy is 0.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 78.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 8 is 0.3984375596046448; The training accuracy is 0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 76.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 9 is 0.39292415976524353; The training accuracy is 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 74.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 10 is 0.3886856436729431; The training accuracy is 0.924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 77.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 11 is 0.3904612064361572; The training accuracy is 0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 78.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 12 is 0.4053955674171448; The training accuracy is 0.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 74.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 13 is 0.38732877373695374; The training accuracy is 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 78.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 14 is 0.3787517249584198; The training accuracy is 0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 64.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 15 is 0.3759597837924957; The training accuracy is 0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 74.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 16 is 0.37242498993873596; The training accuracy is 0.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 77.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 17 is 0.3702971637248993; The training accuracy is 0.943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 77.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 18 is 0.3687030076980591; The training accuracy is 0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:01<00:00, 74.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss at epoch 19 is 0.36853986978530884; The training accuracy is 0.945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_loss = 0\n",
    "epoch_acc = 0\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, target = batch\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        acc = binary_accuracy(outputs, target)\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    print(\"The training loss at epoch {} is {}; The training accuracy is {}\".format(epoch, epoch_loss / len(train_dataloader), \n",
    "                                                                                    round(epoch_acc/len(train_dataloader), 3)))\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T07:54:30.441217Z",
     "iopub.status.busy": "2023-04-27T07:54:30.440631Z",
     "iopub.status.idle": "2023-04-27T07:54:34.587255Z",
     "shell.execute_reply": "2023-04-27T07:54:34.586043Z",
     "shell.execute_reply.started": "2023-04-27T07:54:30.441167Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_prediction(inputs, model):\n",
    "    inputs = torch.tensor(vocab(inputs), dtype=torch.long).to(device).unsqueeze(0)\n",
    "    outpus =  model(inputs).argmax(dim=1).item()\n",
    "    return outpus\n",
    "\n",
    "test['target'] = test['text'].apply(lambda sentence: generate_prediction(sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T07:55:21.552155Z",
     "iopub.status.busy": "2023-04-27T07:55:21.550984Z",
     "iopub.status.idle": "2023-04-27T07:55:21.567502Z",
     "shell.execute_reply": "2023-04-27T07:55:21.565952Z",
     "shell.execute_reply.started": "2023-04-27T07:55:21.552110Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = test[['id', 'target']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

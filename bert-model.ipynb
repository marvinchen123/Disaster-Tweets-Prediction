{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nimport random\nimport pandas as pd\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport string\nfrom torch.utils.data import Dataset, DataLoader\nimport torchtext\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nnltk.download('stopwords')\nnltk.download(\"wordnet\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-30T22:54:25.828207Z","iopub.execute_input":"2023-04-30T22:54:25.828958Z","iopub.status.idle":"2023-04-30T22:54:25.841035Z","shell.execute_reply.started":"2023-04-30T22:54:25.828917Z","shell.execute_reply":"2023-04-30T22:54:25.839823Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-04-30T22:54:26.170209Z","iopub.execute_input":"2023-04-30T22:54:26.170957Z","iopub.status.idle":"2023-04-30T22:54:26.175942Z","shell.execute_reply.started":"2023-04-30T22:54:26.170918Z","shell.execute_reply":"2023-04-30T22:54:26.174640Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"! unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-04-30T22:38:14.078382Z","iopub.execute_input":"2023-04-30T22:38:14.079036Z","iopub.status.idle":"2023-04-30T22:38:15.572121Z","shell.execute_reply.started":"2023-04-30T22:38:14.078998Z","shell.execute_reply":"2023-04-30T22:38:15.570981Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    return seed\n\nseed_everything(42)\nMAX_LENGTH =80\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-04-30T22:54:32.993948Z","iopub.execute_input":"2023-04-30T22:54:32.994655Z","iopub.status.idle":"2023-04-30T22:54:33.003079Z","shell.execute_reply.started":"2023-04-30T22:54:32.994615Z","shell.execute_reply":"2023-04-30T22:54:33.001763Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"train= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest= pd.read_csv(\"../input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-04-30T22:54:35.612782Z","iopub.execute_input":"2023-04-30T22:54:35.613797Z","iopub.status.idle":"2023-04-30T22:54:35.667096Z","shell.execute_reply.started":"2023-04-30T22:54:35.613758Z","shell.execute_reply":"2023-04-30T22:54:35.666126Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r' ', string)\n\ndef clean_sentence(sentence):\n    # remove URLS \n    sentence = re.sub(r'http\\S+', ' ', sentence)\n    # remove emoji's\n    sentence = remove_emoji(sentence)\n    # remove punctuation\n    sentence = re.sub(\"[^0-9A-Za-z ]\", \"\" , sentence)\n    # remove double spaces\n    sentence = sentence.replace('  ',\"\")\n    \n    return sentence.strip()\n\ndef remove_stopwords(tokens, stopwords):\n    clean_token = [word for word in tokens if word not in stopwords]\n    return clean_token\n\n# lemmalization \ndef lemmatize(tokens, lemma):\n    lemmatized_tokens = [lemma.lemmatize(token, pos = 'v') for token in tokens]\n    return lemmatized_tokens","metadata":{"execution":{"iopub.status.busy":"2023-04-30T22:54:37.237404Z","iopub.execute_input":"2023-04-30T22:54:37.238605Z","iopub.status.idle":"2023-04-30T22:54:37.247632Z","shell.execute_reply.started":"2023-04-30T22:54:37.238544Z","shell.execute_reply":"2023-04-30T22:54:37.246085Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')\nlemma = WordNetLemmatizer()\n\ndef processing(df, stopwords, lemma):\n    df['text'] = df['text'].apply(lambda sentence: sentence.lower())\n    df['text'] = df['text'].apply(lambda sentence: clean_sentence(sentence))\n    \n    \nprocessing(train, stopwords, lemma)\nprocessing(test, stopwords, lemma)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T22:54:38.093996Z","iopub.execute_input":"2023-04-30T22:54:38.095064Z","iopub.status.idle":"2023-04-30T22:54:38.198972Z","shell.execute_reply.started":"2023-04-30T22:54:38.095000Z","shell.execute_reply":"2023-04-30T22:54:38.197999Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-30T22:54:39.394025Z","iopub.execute_input":"2023-04-30T22:54:39.394775Z","iopub.status.idle":"2023-04-30T22:54:39.406481Z","shell.execute_reply.started":"2023-04-30T22:54:39.394733Z","shell.execute_reply":"2023-04-30T22:54:39.405475Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  our deeds are the reason of this earthquake ma...   \n1   4     NaN      NaN              forest fire near la ronge sask canada   \n2   5     NaN      NaN  all residents asked to shelter in place are be...   \n3   6     NaN      NaN  13000 people receive wildfires evacuation orde...   \n4   7     NaN      NaN  just got sent this photo from ruby alaska as s...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>our deeds are the reason of this earthquake ma...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>forest fire near la ronge sask canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>all residents asked to shelter in place are be...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13000 people receive wildfires evacuation orde...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>just got sent this photo from ruby alaska as s...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import BertTokenizer\ntoken = BertTokenizer.from_pretrained('bert-large-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-04-30T22:54:42.791433Z","iopub.execute_input":"2023-04-30T22:54:42.794365Z","iopub.status.idle":"2023-04-30T22:54:43.000103Z","shell.execute_reply.started":"2023-04-30T22:54:42.794305Z","shell.execute_reply":"2023-04-30T22:54:42.998990Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel\n\nclass TweetDataset(Dataset):\n    def __init__(self, df):\n        self.x = df['text']\n        self.y = df['target']\n    \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        sentence = self.x[idx]\n        return sentence, self.y[idx]\n    \ntrain_dataset = TweetDataset(train)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T22:54:46.678616Z","iopub.execute_input":"2023-04-30T22:54:46.679379Z","iopub.status.idle":"2023-04-30T22:54:46.686308Z","shell.execute_reply.started":"2023-04-30T22:54:46.679314Z","shell.execute_reply":"2023-04-30T22:54:46.685178Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def collate_fn(data):\n    sentences = []\n    labels = []\n    for item in data:\n        sentences.append(item[0])\n        labels.append(item[1])\n    \n    data = token.batch_encode_plus(batch_text_or_text_pairs=sentences,\n                                   truncation=True,\n                                   padding='max_length',\n                                   max_length=80,\n                                   return_tensors='pt')\n    input_ids = data['input_ids'].to(device)\n    attention_mask = data['attention_mask'].to(device)\n    token_type_ids = data['token_type_ids'].to(device)\n    labels = torch.LongTensor(labels).to(device)\n    return input_ids, attention_mask, token_type_ids, labels","metadata":{"execution":{"iopub.status.busy":"2023-04-30T22:54:48.667381Z","iopub.execute_input":"2023-04-30T22:54:48.668526Z","iopub.status.idle":"2023-04-30T22:54:48.676362Z","shell.execute_reply.started":"2023-04-30T22:54:48.668477Z","shell.execute_reply":"2023-04-30T22:54:48.674709Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(dataset=train_dataset,\n                          batch_size=32,\n                          collate_fn=collate_fn,\n                          shuffle=True,\n                          drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T22:54:48.854250Z","iopub.execute_input":"2023-04-30T22:54:48.854882Z","iopub.status.idle":"2023-04-30T22:54:48.860145Z","shell.execute_reply.started":"2023-04-30T22:54:48.854845Z","shell.execute_reply":"2023-04-30T22:54:48.858870Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel\npretrained = BertModel.from_pretrained('bert-large-uncased')\npretrained.to(device)\n\nfor param in pretrained.parameters():\n    param.requires_grad_(False)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T23:11:26.747854Z","iopub.execute_input":"2023-04-30T23:11:26.748236Z","iopub.status.idle":"2023-04-30T23:11:31.622978Z","shell.execute_reply.started":"2023-04-30T23:11:26.748200Z","shell.execute_reply":"2023-04-30T23:11:31.621883Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"class bert_model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(1024, 2)\n    \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        with torch.no_grad():\n            out = pretrained(input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids).last_hidden_state[:, 0]\n        # out = self.dropout(out)\n        out = self.fc(out).softmax(dim=1)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-04-30T23:11:31.625098Z","iopub.execute_input":"2023-04-30T23:11:31.625418Z","iopub.status.idle":"2023-04-30T23:11:31.631712Z","shell.execute_reply.started":"2023-04-30T23:11:31.625389Z","shell.execute_reply":"2023-04-30T23:11:31.630405Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def binary_accuracy(preds, y):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    \"\"\"\n    prediction = preds.argmax(dim=1)\n    correct = (prediction == y).float() #convert into float for division \n    acc = correct.sum() / len(correct)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2023-04-30T23:11:31.633201Z","iopub.execute_input":"2023-04-30T23:11:31.633866Z","iopub.status.idle":"2023-04-30T23:11:31.644373Z","shell.execute_reply.started":"2023-04-30T23:11:31.633829Z","shell.execute_reply":"2023-04-30T23:11:31.643358Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model = bert_model()\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.Adam(model.parameters(), lr = 0.0005)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T23:11:31.648069Z","iopub.execute_input":"2023-04-30T23:11:31.648383Z","iopub.status.idle":"2023-04-30T23:11:31.656269Z","shell.execute_reply.started":"2023-04-30T23:11:31.648314Z","shell.execute_reply":"2023-04-30T23:11:31.655283Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"epoch_loss = 0\nepoch_acc = 0\nfor epoch in range(10):\n    model.train()\n    optimizer.zero_grad()\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n        input_ids, attention_mask, token_type_ids, labels = batch\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss\n        \n        acc = binary_accuracy(outputs, labels)\n        epoch_acc += acc.item()\n    \n    print(\"The training loss at epoch {} is {}; The training accuracy is {}\".format(epoch, epoch_loss / len(train_loader), \n                                                                                    round(epoch_acc/len(train_loader), 3)))\n    epoch_loss = 0\n    epoch_acc = 0","metadata":{"execution":{"iopub.status.busy":"2023-04-30T23:11:31.657946Z","iopub.execute_input":"2023-04-30T23:11:31.658297Z","iopub.status.idle":"2023-04-30T23:21:58.969795Z","shell.execute_reply.started":"2023-04-30T23:11:31.658262Z","shell.execute_reply":"2023-04-30T23:21:58.968688Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"100%|██████████| 237/237 [01:02<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 0 is 0.5720729827880859; The training accuracy is 0.748\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 237/237 [01:02<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 1 is 0.5280571579933167; The training accuracy is 0.789\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 237/237 [01:02<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 2 is 0.5162314176559448; The training accuracy is 0.798\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 237/237 [01:02<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 3 is 0.508943498134613; The training accuracy is 0.803\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 237/237 [01:02<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 4 is 0.5035176277160645; The training accuracy is 0.807\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 237/237 [01:02<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 5 is 0.5005767941474915; The training accuracy is 0.811\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 237/237 [01:02<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 6 is 0.498150110244751; The training accuracy is 0.814\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 237/237 [01:02<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 7 is 0.49499326944351196; The training accuracy is 0.816\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 237/237 [01:02<00:00,  3.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 8 is 0.49276334047317505; The training accuracy is 0.818\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 237/237 [01:02<00:00,  3.77it/s]","output_type":"stream"},{"name":"stdout","text":"The training loss at epoch 9 is 0.49039047956466675; The training accuracy is 0.82\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"test_set: 0.8","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# prediction","metadata":{}},{"cell_type":"code","source":"data = token.batch_encode_plus(batch_text_or_text_pairs=test['text'], truncation=True, padding='max_length', max_length=30, return_tensors='pt')\ninput_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\ntoken_type_ids = data['token_type_ids'].to(device)\nprediction = model(input_ids, attention_mask, token_type_ids).argmax(dim=1).cpu()","metadata":{"execution":{"iopub.status.busy":"2023-04-30T23:22:29.578146Z","iopub.execute_input":"2023-04-30T23:22:29.578582Z","iopub.status.idle":"2023-04-30T23:22:39.264503Z","shell.execute_reply.started":"2023-04-30T23:22:29.578542Z","shell.execute_reply":"2023-04-30T23:22:39.263383Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"test['target'] = prediction\nsubmission = test[['id', 'target']]\nsubmission.to_csv('submission2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T23:22:39.266427Z","iopub.execute_input":"2023-04-30T23:22:39.266775Z","iopub.status.idle":"2023-04-30T23:22:39.283318Z","shell.execute_reply.started":"2023-04-30T23:22:39.266744Z","shell.execute_reply":"2023-04-30T23:22:39.282295Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}